{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0XHmZjEv6xul"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "vfepOzSh7Isq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-tuner\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY_1tZDn7Rph",
        "outputId": "a8b73954-d157-49be-a364-67c69ad37ea9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m112.6/129.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt"
      ],
      "metadata": {
        "id": "NOvMH00S7bbn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "rhSY36Ev7i9h"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRUjpPYV9lri",
        "outputId": "9ef9d0e5-f982-4a2d-8c1c-eec0e25e3737"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMNiC0WT-bYH",
        "outputId": "da2fbf64-a81e-459e-d44e-e66f874ff752"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xIRXebRq99nJ",
        "outputId": "2430b1ca-f20f-4244-f517-21900490ec67"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
              "          1,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
              "          0,   3],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
              "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
              "         10,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
              "         72,  15],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
              "        172,  66],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
              "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
              "        229,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
              "        173,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
              "        202,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
              "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
              "        209,  52],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
              "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
              "        167,  56],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
              "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
              "         92,   0],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
              "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
              "         77,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
              "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
              "        159,   0],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
              "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
              "        215,   0],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
              "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
              "        246,   0],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
              "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
              "        225,   0],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
              "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
              "        229,  29],\n",
              "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
              "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
              "        230,  67],\n",
              "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
              "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
              "        206, 115],\n",
              "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
              "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
              "        210,  92],\n",
              "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
              "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
              "        170,   0],\n",
              "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
              "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-b46d3296-512d-44dc-b0cc-c4db3ff4a683\" class=\"ndarray_repr\"><pre>ndarray (28, 28) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACBUlEQVR4nLXSz0tUURQH8O+597373rx545g6NmNJY6TlJgozEsKFZGCrkDBoVdGmdf9BiwJ3bVr1PwhBUbQfJQcKjWZRTsjo0KDpDPV+eN+997WK0Z2bvtsPfM+Bc4D/HwJycwBZAEAE6zAyfe5RFMQfFYiRAj+CXM/c2HK82VetVMM34RGUmCxz9v7yYnW9dnWyskyHR6azi72Jwep3ScVk9c7LLhKQrpRBSiI2n76puZFT3doUwH4pcmw/zpjrU2zw3dFt4XEWdvbKhpinzTBYt5bDH4qlLYO8cbKBWHOudDHluFtiLrLD0kmM2//6q9VFS+JLLDxv9GzMPV9v3XuzYgHEGSUGCngbRCLd4W6CxPCLHTDwVMkDA0y/qEa/lFFBoF2EEME8CAD6hsZK8+djlmSatuiXXsWfNh27NU6Yelro1bytPElRbaGaO1FGPfc7zPg9HvHlIaVD8AjID9y/+bgZ/6iP9ks7J/QZevh8w/cd2PlGs8CKt92R7MQEk0yA7GtWq9ETN3zRs7fpR7FaWi/3yXaipW1IjFnbaSM70N7dsRzbzbHd8aCx7+wmKskUO5esz0sPmvXYF67gBzoNfxptxb5stxM10iLg1pOTO23NhcUptW1hE6gFYYprC8QMMPNsMM+4pamVbps/HGkSsg+1Cv4d+0Jh//Sm3DjGix4rfwFoJNh2/0cDFgAAAABJRU5ErkJggg==\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
              "          1,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
              "          0,   3],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
              "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
              "         10,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
              "         72,  15],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
              "        172,  66],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
              "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
              "        229,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
              "        173,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
              "        202,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
              "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
              "        209,  52],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
              "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
              "        167,  56],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
              "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
              "         92,   0],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
              "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
              "         77,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
              "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
              "        159,   0],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
              "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
              "        215,   0],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
              "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
              "        246,   0],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
              "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
              "        225,   0],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
              "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
              "        229,  29],\n",
              "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
              "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
              "        230,  67],\n",
              "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
              "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
              "        206, 115],\n",
              "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
              "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
              "        210,  92],\n",
              "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
              "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
              "        170,   0],\n",
              "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
              "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-b46d3296-512d-44dc-b0cc-c4db3ff4a683 button').onclick = (e) => {\n",
              "        document.querySelector('#id-b46d3296-512d-44dc-b0cc-c4db3ff4a683').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-b46d3296-512d-44dc-b0cc-c4db3ff4a683 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_train[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue5h1B2J-n1s",
        "outputId": "844cf38e-a7ba-49f5-9416-886b0e33a9e5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(img_train[0], cmap = 'binary')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "RR3X--Hx-AeZ",
        "outputId": "1dd44397-34e4-4b1b-ad6f-929d2f315f46"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize pixel values between 0 to 1\n",
        "img_train = img_train.astype('float32')/255.0\n",
        "img_test = img_test.astype('float32')/255.0"
      ],
      "metadata": {
        "id": "XazvwCmB_Xnl"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x30l-FVCLaNE",
        "outputId": "1501e3f5-75e3-4a7e-c8bd-b87e8de1851f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.00392157, 0.        , 0.        ,\n",
              "        0.05098039, 0.28627452, 0.        , 0.        , 0.00392157,\n",
              "        0.01568628, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00392157, 0.00392157, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.01176471, 0.        , 0.14117648,\n",
              "        0.53333336, 0.49803922, 0.24313726, 0.21176471, 0.        ,\n",
              "        0.        , 0.        , 0.00392157, 0.01176471, 0.01568628,\n",
              "        0.        , 0.        , 0.01176471],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.02352941, 0.        , 0.4       ,\n",
              "        0.8       , 0.6901961 , 0.5254902 , 0.5647059 , 0.48235294,\n",
              "        0.09019608, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.04705882, 0.03921569, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.60784316,\n",
              "        0.9254902 , 0.8117647 , 0.69803923, 0.41960785, 0.6117647 ,\n",
              "        0.6313726 , 0.42745098, 0.2509804 , 0.09019608, 0.3019608 ,\n",
              "        0.50980395, 0.28235295, 0.05882353],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.00392157, 0.        , 0.27058825, 0.8117647 ,\n",
              "        0.8745098 , 0.85490197, 0.84705883, 0.84705883, 0.6392157 ,\n",
              "        0.49803922, 0.4745098 , 0.47843137, 0.57254905, 0.5529412 ,\n",
              "        0.34509805, 0.6745098 , 0.25882354],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
              "        0.00392157, 0.00392157, 0.        , 0.78431374, 0.9098039 ,\n",
              "        0.9098039 , 0.9137255 , 0.8980392 , 0.8745098 , 0.8745098 ,\n",
              "        0.84313726, 0.8352941 , 0.6431373 , 0.49803922, 0.48235294,\n",
              "        0.76862746, 0.8980392 , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.7176471 , 0.88235295,\n",
              "        0.84705883, 0.8745098 , 0.89411765, 0.92156863, 0.8901961 ,\n",
              "        0.8784314 , 0.87058824, 0.8784314 , 0.8666667 , 0.8745098 ,\n",
              "        0.9607843 , 0.6784314 , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.75686276, 0.89411765,\n",
              "        0.85490197, 0.8352941 , 0.7764706 , 0.7058824 , 0.83137256,\n",
              "        0.8235294 , 0.827451  , 0.8352941 , 0.8745098 , 0.8627451 ,\n",
              "        0.9529412 , 0.7921569 , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
              "        0.01176471, 0.        , 0.04705882, 0.85882354, 0.8627451 ,\n",
              "        0.83137256, 0.85490197, 0.7529412 , 0.6627451 , 0.8901961 ,\n",
              "        0.8156863 , 0.85490197, 0.8784314 , 0.83137256, 0.8862745 ,\n",
              "        0.77254903, 0.81960785, 0.20392157],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.02352941, 0.        , 0.3882353 , 0.95686275, 0.87058824,\n",
              "        0.8627451 , 0.85490197, 0.79607844, 0.7764706 , 0.8666667 ,\n",
              "        0.84313726, 0.8352941 , 0.87058824, 0.8627451 , 0.9607843 ,\n",
              "        0.46666667, 0.654902  , 0.21960784],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.01568628,\n",
              "        0.        , 0.        , 0.21568628, 0.9254902 , 0.89411765,\n",
              "        0.9019608 , 0.89411765, 0.9411765 , 0.9098039 , 0.8352941 ,\n",
              "        0.85490197, 0.8745098 , 0.91764706, 0.8509804 , 0.8509804 ,\n",
              "        0.81960785, 0.36078432, 0.        ],\n",
              "       [0.        , 0.        , 0.00392157, 0.01568628, 0.02352941,\n",
              "        0.02745098, 0.00784314, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.92941177, 0.8862745 , 0.8509804 ,\n",
              "        0.8745098 , 0.87058824, 0.85882354, 0.87058824, 0.8666667 ,\n",
              "        0.84705883, 0.8745098 , 0.8980392 , 0.84313726, 0.85490197,\n",
              "        1.        , 0.3019608 , 0.        ],\n",
              "       [0.        , 0.01176471, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.24313726,\n",
              "        0.5686275 , 0.8       , 0.89411765, 0.8117647 , 0.8352941 ,\n",
              "        0.8666667 , 0.85490197, 0.8156863 , 0.827451  , 0.85490197,\n",
              "        0.8784314 , 0.8745098 , 0.85882354, 0.84313726, 0.8784314 ,\n",
              "        0.95686275, 0.62352943, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.07058824,\n",
              "        0.17254902, 0.32156864, 0.41960785, 0.7411765 , 0.89411765,\n",
              "        0.8627451 , 0.87058824, 0.8509804 , 0.8862745 , 0.78431374,\n",
              "        0.8039216 , 0.827451  , 0.9019608 , 0.8784314 , 0.91764706,\n",
              "        0.6901961 , 0.7372549 , 0.98039216, 0.972549  , 0.9137255 ,\n",
              "        0.93333334, 0.84313726, 0.        ],\n",
              "       [0.        , 0.22352941, 0.73333335, 0.8156863 , 0.8784314 ,\n",
              "        0.8666667 , 0.8784314 , 0.8156863 , 0.8       , 0.8392157 ,\n",
              "        0.8156863 , 0.81960785, 0.78431374, 0.62352943, 0.9607843 ,\n",
              "        0.75686276, 0.80784315, 0.8745098 , 1.        , 1.        ,\n",
              "        0.8666667 , 0.91764706, 0.8666667 , 0.827451  , 0.8627451 ,\n",
              "        0.9098039 , 0.9647059 , 0.        ],\n",
              "       [0.01176471, 0.7921569 , 0.89411765, 0.8784314 , 0.8666667 ,\n",
              "        0.827451  , 0.827451  , 0.8392157 , 0.8039216 , 0.8039216 ,\n",
              "        0.8039216 , 0.8627451 , 0.9411765 , 0.3137255 , 0.5882353 ,\n",
              "        1.        , 0.8980392 , 0.8666667 , 0.7372549 , 0.6039216 ,\n",
              "        0.7490196 , 0.8235294 , 0.8       , 0.81960785, 0.87058824,\n",
              "        0.89411765, 0.88235295, 0.        ],\n",
              "       [0.38431373, 0.9137255 , 0.7764706 , 0.8235294 , 0.87058824,\n",
              "        0.8980392 , 0.8980392 , 0.91764706, 0.9764706 , 0.8627451 ,\n",
              "        0.7607843 , 0.84313726, 0.8509804 , 0.94509804, 0.25490198,\n",
              "        0.28627452, 0.41568628, 0.45882353, 0.65882355, 0.85882354,\n",
              "        0.8666667 , 0.84313726, 0.8509804 , 0.8745098 , 0.8745098 ,\n",
              "        0.8784314 , 0.8980392 , 0.11372549],\n",
              "       [0.29411766, 0.8       , 0.83137256, 0.8       , 0.75686276,\n",
              "        0.8039216 , 0.827451  , 0.88235295, 0.84705883, 0.7254902 ,\n",
              "        0.77254903, 0.80784315, 0.7764706 , 0.8352941 , 0.9411765 ,\n",
              "        0.7647059 , 0.8901961 , 0.9607843 , 0.9372549 , 0.8745098 ,\n",
              "        0.85490197, 0.83137256, 0.81960785, 0.87058824, 0.8627451 ,\n",
              "        0.8666667 , 0.9019608 , 0.2627451 ],\n",
              "       [0.1882353 , 0.79607844, 0.7176471 , 0.7607843 , 0.8352941 ,\n",
              "        0.77254903, 0.7254902 , 0.74509805, 0.7607843 , 0.7529412 ,\n",
              "        0.7921569 , 0.8392157 , 0.85882354, 0.8666667 , 0.8627451 ,\n",
              "        0.9254902 , 0.88235295, 0.84705883, 0.78039217, 0.80784315,\n",
              "        0.7294118 , 0.70980394, 0.69411767, 0.6745098 , 0.70980394,\n",
              "        0.8039216 , 0.80784315, 0.4509804 ],\n",
              "       [0.        , 0.47843137, 0.85882354, 0.75686276, 0.7019608 ,\n",
              "        0.67058825, 0.7176471 , 0.76862746, 0.8       , 0.8235294 ,\n",
              "        0.8352941 , 0.8117647 , 0.827451  , 0.8235294 , 0.78431374,\n",
              "        0.76862746, 0.7607843 , 0.7490196 , 0.7647059 , 0.7490196 ,\n",
              "        0.7764706 , 0.7529412 , 0.6901961 , 0.6117647 , 0.654902  ,\n",
              "        0.69411767, 0.8235294 , 0.36078432],\n",
              "       [0.        , 0.        , 0.2901961 , 0.7411765 , 0.83137256,\n",
              "        0.7490196 , 0.6862745 , 0.6745098 , 0.6862745 , 0.70980394,\n",
              "        0.7254902 , 0.7372549 , 0.7411765 , 0.7372549 , 0.75686276,\n",
              "        0.7764706 , 0.8       , 0.81960785, 0.8235294 , 0.8235294 ,\n",
              "        0.827451  , 0.7372549 , 0.7372549 , 0.7607843 , 0.7529412 ,\n",
              "        0.84705883, 0.6666667 , 0.        ],\n",
              "       [0.00784314, 0.        , 0.        , 0.        , 0.25882354,\n",
              "        0.78431374, 0.87058824, 0.92941177, 0.9372549 , 0.9490196 ,\n",
              "        0.9647059 , 0.9529412 , 0.95686275, 0.8666667 , 0.8627451 ,\n",
              "        0.75686276, 0.7490196 , 0.7019608 , 0.7137255 , 0.7137255 ,\n",
              "        0.70980394, 0.6901961 , 0.6509804 , 0.65882355, 0.3882353 ,\n",
              "        0.22745098, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.15686275, 0.23921569, 0.17254902,\n",
              "        0.28235295, 0.16078432, 0.13725491, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp):\n",
        "  model = keras.Sequential()\n",
        "  #input layers\n",
        "  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
        "\n",
        "  # Tune the number of units in the first Dense layer\n",
        "  # Choose an optimal value between 32-512\n",
        "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "  model.add(keras.layers.Dense(10))#output layer\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "s3lcnFj6VeWx"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp):\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  model.add(keras.layers.Flatten(input_shape=(28,28)))   ## Input layer 28 * 28 = 784 pixels so total 784 input after flattering the 28*28 image.\n",
        "\n",
        "  # hp.int for ranges and hp.choice for given choices\n",
        "\n",
        "  hp_units = hp.Int('units', min_value = 32, max_value = 512, step =32)\n",
        "  model.add(keras.layers.Dense(units = hp_units, activation = 'relu'))    ## Hidden layer\n",
        "  model.add(keras.layers.Dense(10))   ## output layer\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3 , 1e-4] )\n",
        "\n",
        "  model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
        "                loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "63Go8I3gLgzu"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for hyperparameter tuning\n",
        "\n",
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective = 'val_accuracy',\n",
        "                     max_epochs = 10,\n",
        "                     factor = 3,\n",
        "                     directory = 'my_dir',\n",
        "                     project_name = 'intro_to_kt')"
      ],
      "metadata": {
        "id": "yOslNzFbRGgq"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor ='val_loss', patience = 5)\n",
        "\n",
        "# Patience = 5 menas till 5 iteration/epocs  if I am not getting any changes  it will stop over there"
      ],
      "metadata": {
        "id": "mFVejdVNVCKJ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(img_train, label_train, epochs=50, validation_split=0.2,callbacks=[stop_early])\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBNPareOW10z",
        "outputId": "f6311e40-079e-4578-f77a-36b83f0eb5a2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 02m 04s]\n",
            "val_accuracy: 0.890250027179718\n",
            "\n",
            "Best val_accuracy So Far: 0.8924166560173035\n",
            "Total elapsed time: 00h 18m 52s\n",
            "\n",
            "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
            "layer is 512 and the optimal learning rate for the optimizer\n",
            "is 0.001.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_hps.get('units')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fiyKxiGhiaL",
        "outputId": "5bd077c8-c115-476d-87c1-342c5dbe9668"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(img_train, label_train, epochs=50, validation_split=0.2)\n",
        "\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka0fvEOgWjr2",
        "outputId": "7bb89a80-e180-42cd-cc3e-fa18ebb49869"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.7804 - loss: 0.6161 - val_accuracy: 0.8490 - val_loss: 0.4135\n",
            "Epoch 2/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.8651 - loss: 0.3748 - val_accuracy: 0.8668 - val_loss: 0.3650\n",
            "Epoch 3/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 0.8794 - loss: 0.3261 - val_accuracy: 0.8783 - val_loss: 0.3368\n",
            "Epoch 4/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.8860 - loss: 0.3069 - val_accuracy: 0.8753 - val_loss: 0.3501\n",
            "Epoch 5/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.8948 - loss: 0.2825 - val_accuracy: 0.8780 - val_loss: 0.3348\n",
            "Epoch 6/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 7ms/step - accuracy: 0.9019 - loss: 0.2606 - val_accuracy: 0.8886 - val_loss: 0.3116\n",
            "Epoch 7/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9016 - loss: 0.2577 - val_accuracy: 0.8909 - val_loss: 0.3076\n",
            "Epoch 8/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9127 - loss: 0.2359 - val_accuracy: 0.8910 - val_loss: 0.3090\n",
            "Epoch 9/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9138 - loss: 0.2279 - val_accuracy: 0.8878 - val_loss: 0.3273\n",
            "Epoch 10/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9138 - loss: 0.2278 - val_accuracy: 0.8912 - val_loss: 0.3167\n",
            "Epoch 11/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.9187 - loss: 0.2167 - val_accuracy: 0.8908 - val_loss: 0.3171\n",
            "Epoch 12/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.9260 - loss: 0.2022 - val_accuracy: 0.8906 - val_loss: 0.3231\n",
            "Epoch 13/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9257 - loss: 0.2000 - val_accuracy: 0.8892 - val_loss: 0.3357\n",
            "Epoch 14/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9325 - loss: 0.1819 - val_accuracy: 0.8950 - val_loss: 0.3203\n",
            "Epoch 15/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9338 - loss: 0.1766 - val_accuracy: 0.8856 - val_loss: 0.3538\n",
            "Epoch 16/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9343 - loss: 0.1731 - val_accuracy: 0.8904 - val_loss: 0.3497\n",
            "Epoch 17/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9367 - loss: 0.1661 - val_accuracy: 0.8903 - val_loss: 0.3464\n",
            "Epoch 18/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9377 - loss: 0.1636 - val_accuracy: 0.8935 - val_loss: 0.3423\n",
            "Epoch 19/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9428 - loss: 0.1527 - val_accuracy: 0.8925 - val_loss: 0.3584\n",
            "Epoch 20/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 7ms/step - accuracy: 0.9455 - loss: 0.1493 - val_accuracy: 0.8900 - val_loss: 0.3785\n",
            "Epoch 21/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9441 - loss: 0.1478 - val_accuracy: 0.8879 - val_loss: 0.3766\n",
            "Epoch 22/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.9456 - loss: 0.1411 - val_accuracy: 0.8878 - val_loss: 0.3828\n",
            "Epoch 23/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9482 - loss: 0.1396 - val_accuracy: 0.8912 - val_loss: 0.3820\n",
            "Epoch 24/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 8ms/step - accuracy: 0.9488 - loss: 0.1351 - val_accuracy: 0.8853 - val_loss: 0.4283\n",
            "Epoch 25/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 10ms/step - accuracy: 0.9512 - loss: 0.1291 - val_accuracy: 0.8942 - val_loss: 0.3816\n",
            "Epoch 26/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.9530 - loss: 0.1231 - val_accuracy: 0.8978 - val_loss: 0.3744\n",
            "Epoch 27/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.9508 - loss: 0.1282 - val_accuracy: 0.8865 - val_loss: 0.4067\n",
            "Epoch 28/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9538 - loss: 0.1209 - val_accuracy: 0.8913 - val_loss: 0.4220\n",
            "Epoch 29/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 7ms/step - accuracy: 0.9553 - loss: 0.1197 - val_accuracy: 0.8913 - val_loss: 0.4255\n",
            "Epoch 30/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9574 - loss: 0.1165 - val_accuracy: 0.8878 - val_loss: 0.4297\n",
            "Epoch 31/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9590 - loss: 0.1076 - val_accuracy: 0.8856 - val_loss: 0.4237\n",
            "Epoch 32/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.9617 - loss: 0.1051 - val_accuracy: 0.8946 - val_loss: 0.4256\n",
            "Epoch 33/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.9603 - loss: 0.1051 - val_accuracy: 0.8915 - val_loss: 0.4442\n",
            "Epoch 34/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9619 - loss: 0.0999 - val_accuracy: 0.8927 - val_loss: 0.4291\n",
            "Epoch 35/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9627 - loss: 0.0983 - val_accuracy: 0.8891 - val_loss: 0.4537\n",
            "Epoch 36/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.9631 - loss: 0.0952 - val_accuracy: 0.8956 - val_loss: 0.4449\n",
            "Epoch 37/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.9633 - loss: 0.0959 - val_accuracy: 0.8942 - val_loss: 0.4435\n",
            "Epoch 38/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9662 - loss: 0.0920 - val_accuracy: 0.8923 - val_loss: 0.4568\n",
            "Epoch 39/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9660 - loss: 0.0933 - val_accuracy: 0.8924 - val_loss: 0.4606\n",
            "Epoch 40/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9688 - loss: 0.0845 - val_accuracy: 0.8954 - val_loss: 0.4657\n",
            "Epoch 41/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9666 - loss: 0.0891 - val_accuracy: 0.8934 - val_loss: 0.4930\n",
            "Epoch 42/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9668 - loss: 0.0874 - val_accuracy: 0.8938 - val_loss: 0.4949\n",
            "Epoch 43/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.9676 - loss: 0.0865 - val_accuracy: 0.8944 - val_loss: 0.5340\n",
            "Epoch 44/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9692 - loss: 0.0810 - val_accuracy: 0.8820 - val_loss: 0.5378\n",
            "Epoch 45/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.9672 - loss: 0.0860 - val_accuracy: 0.8924 - val_loss: 0.5045\n",
            "Epoch 46/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9693 - loss: 0.0824 - val_accuracy: 0.8887 - val_loss: 0.5351\n",
            "Epoch 47/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9712 - loss: 0.0803 - val_accuracy: 0.8886 - val_loss: 0.5436\n",
            "Epoch 48/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9717 - loss: 0.0759 - val_accuracy: 0.8923 - val_loss: 0.5538\n",
            "Epoch 49/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9698 - loss: 0.0777 - val_accuracy: 0.8891 - val_loss: 0.5584\n",
            "Epoch 50/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.9730 - loss: 0.0718 - val_accuracy: 0.8985 - val_loss: 0.5024\n",
            "Best epoch: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = model.evaluate(img_test, label_test)\n",
        "print(\"[test loss, test accuracy]:\", eval_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuDY78gek9CQ",
        "outputId": "81fef47e-d25a-4dec-92da-181d5713dffc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8926 - loss: 0.5871\n",
            "[test loss, test accuracy]: [0.5811290740966797, 0.8927000164985657]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fYfJ5Ohik9NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Retrain the model\n",
        "hypermodel.fit(img_train, label_train, epochs=best_epoch, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAlcK_GmiU_g",
        "outputId": "213fb03b-14d5-436e-9c2c-f4d8ab543f98"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.7856 - loss: 0.6119 - val_accuracy: 0.8607 - val_loss: 0.3896\n",
            "Epoch 2/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.8634 - loss: 0.3789 - val_accuracy: 0.8658 - val_loss: 0.3668\n",
            "Epoch 3/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - accuracy: 0.8754 - loss: 0.3307 - val_accuracy: 0.8684 - val_loss: 0.3553\n",
            "Epoch 4/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.8840 - loss: 0.3117 - val_accuracy: 0.8727 - val_loss: 0.3469\n",
            "Epoch 5/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.8946 - loss: 0.2866 - val_accuracy: 0.8795 - val_loss: 0.3332\n",
            "Epoch 6/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9008 - loss: 0.2670 - val_accuracy: 0.8894 - val_loss: 0.3157\n",
            "Epoch 7/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9083 - loss: 0.2472 - val_accuracy: 0.8790 - val_loss: 0.3500\n",
            "Epoch 8/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.9103 - loss: 0.2400 - val_accuracy: 0.8883 - val_loss: 0.3170\n",
            "Epoch 9/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.9149 - loss: 0.2261 - val_accuracy: 0.8918 - val_loss: 0.3076\n",
            "Epoch 10/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.9163 - loss: 0.2226 - val_accuracy: 0.8848 - val_loss: 0.3378\n",
            "Epoch 11/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9212 - loss: 0.2121 - val_accuracy: 0.8880 - val_loss: 0.3144\n",
            "Epoch 12/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.9221 - loss: 0.2051 - val_accuracy: 0.8928 - val_loss: 0.3045\n",
            "Epoch 13/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9261 - loss: 0.1926 - val_accuracy: 0.8917 - val_loss: 0.3217\n",
            "Epoch 14/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9297 - loss: 0.1866 - val_accuracy: 0.8905 - val_loss: 0.3327\n",
            "Epoch 15/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9326 - loss: 0.1800 - val_accuracy: 0.8970 - val_loss: 0.3205\n",
            "Epoch 16/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9345 - loss: 0.1762 - val_accuracy: 0.8898 - val_loss: 0.3392\n",
            "Epoch 17/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9374 - loss: 0.1670 - val_accuracy: 0.8964 - val_loss: 0.3224\n",
            "Epoch 18/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9404 - loss: 0.1606 - val_accuracy: 0.8943 - val_loss: 0.3463\n",
            "Epoch 19/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 8ms/step - accuracy: 0.9424 - loss: 0.1525 - val_accuracy: 0.8940 - val_loss: 0.3435\n",
            "Epoch 20/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9423 - loss: 0.1543 - val_accuracy: 0.8973 - val_loss: 0.3441\n",
            "Epoch 21/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 7ms/step - accuracy: 0.9457 - loss: 0.1430 - val_accuracy: 0.8944 - val_loss: 0.3514\n",
            "Epoch 22/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.9473 - loss: 0.1389 - val_accuracy: 0.8902 - val_loss: 0.3638\n",
            "Epoch 23/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9473 - loss: 0.1393 - val_accuracy: 0.8875 - val_loss: 0.4009\n",
            "Epoch 24/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9478 - loss: 0.1390 - val_accuracy: 0.8959 - val_loss: 0.3619\n",
            "Epoch 25/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 7ms/step - accuracy: 0.9530 - loss: 0.1261 - val_accuracy: 0.8871 - val_loss: 0.4007\n",
            "Epoch 26/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9521 - loss: 0.1269 - val_accuracy: 0.8918 - val_loss: 0.3708\n",
            "Epoch 27/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 7ms/step - accuracy: 0.9538 - loss: 0.1234 - val_accuracy: 0.8963 - val_loss: 0.3739\n",
            "Epoch 28/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9558 - loss: 0.1188 - val_accuracy: 0.8967 - val_loss: 0.3918\n",
            "Epoch 29/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9571 - loss: 0.1166 - val_accuracy: 0.8892 - val_loss: 0.4097\n",
            "Epoch 30/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9570 - loss: 0.1151 - val_accuracy: 0.8913 - val_loss: 0.3999\n",
            "Epoch 31/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.9580 - loss: 0.1132 - val_accuracy: 0.8957 - val_loss: 0.3950\n",
            "Epoch 32/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 11ms/step - accuracy: 0.9600 - loss: 0.1081 - val_accuracy: 0.8865 - val_loss: 0.4638\n",
            "Epoch 33/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9592 - loss: 0.1066 - val_accuracy: 0.8963 - val_loss: 0.4150\n",
            "Epoch 34/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.9589 - loss: 0.1065 - val_accuracy: 0.8946 - val_loss: 0.4326\n",
            "Epoch 35/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9637 - loss: 0.0989 - val_accuracy: 0.8932 - val_loss: 0.4237\n",
            "Epoch 36/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.9636 - loss: 0.0952 - val_accuracy: 0.8895 - val_loss: 0.4762\n",
            "Epoch 37/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9633 - loss: 0.0980 - val_accuracy: 0.8863 - val_loss: 0.4724\n",
            "Epoch 38/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.9643 - loss: 0.0931 - val_accuracy: 0.8942 - val_loss: 0.4425\n",
            "Epoch 39/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9616 - loss: 0.0991 - val_accuracy: 0.8959 - val_loss: 0.4515\n",
            "Epoch 40/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9672 - loss: 0.0876 - val_accuracy: 0.8902 - val_loss: 0.4769\n",
            "Epoch 41/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9671 - loss: 0.0867 - val_accuracy: 0.8921 - val_loss: 0.5170\n",
            "Epoch 42/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9683 - loss: 0.0876 - val_accuracy: 0.8914 - val_loss: 0.4992\n",
            "Epoch 43/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9679 - loss: 0.0855 - val_accuracy: 0.8930 - val_loss: 0.4698\n",
            "Epoch 44/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9684 - loss: 0.0821 - val_accuracy: 0.8883 - val_loss: 0.5182\n",
            "Epoch 45/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.9697 - loss: 0.0790 - val_accuracy: 0.8868 - val_loss: 0.4978\n",
            "Epoch 46/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9695 - loss: 0.0817 - val_accuracy: 0.8924 - val_loss: 0.5101\n",
            "Epoch 47/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9695 - loss: 0.0824 - val_accuracy: 0.8942 - val_loss: 0.5261\n",
            "Epoch 48/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.9707 - loss: 0.0783 - val_accuracy: 0.8928 - val_loss: 0.5408\n",
            "Epoch 49/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9727 - loss: 0.0720 - val_accuracy: 0.8899 - val_loss: 0.5597\n",
            "Epoch 50/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9749 - loss: 0.0692 - val_accuracy: 0.8956 - val_loss: 0.5372\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e4df32c8f70>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = hypermodel.evaluate(img_test, label_test)\n",
        "print(\"[test loss, test accuracy]:\", eval_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tq28hikiVC3",
        "outputId": "b890942c-ddb5-4ac2-fef8-24c16c7f9889"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8882 - loss: 0.5779\n",
            "[test loss, test accuracy]: [0.5831392407417297, 0.8894000053405762]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(img_test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "65TNDsfsjX0_",
        "outputId": "cf2f6d04-b605-42af-9459-3167ba4d242e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(28, 28), dtype=float32). Expected shape (None, 28, 28), but input has incompatible shape (28, 28)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(28, 28), dtype=float32)\n  • training=False\n  • mask=None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-383ec578a85a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36m_adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    242\u001b[0m                     \u001b[0madjusted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0;34mf\"Invalid input shape for input {x}. Expected shape \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0;34mf\"{ref_shape}, but input has incompatible shape {x.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(28, 28), dtype=float32). Expected shape (None, 28, 28), but input has incompatible shape (28, 28)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(28, 28), dtype=float32)\n  • training=False\n  • mask=None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Add a batch dimension to the single image\n",
        "input_image = np.expand_dims(img_test[0], axis=0)  # Shape becomes (1, 28, 28)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(input_image)\n",
        "\n",
        "# Interpret the prediction\n",
        "predicted_class = np.argmax(predictions[0])  # Get the class with the highest probability\n",
        "print(f\"Predicted class for the image: {predicted_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JopywledoVbl",
        "outputId": "14e17cbd-6ffc-4462-e575-01fcd548fc86"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "Predicted class for the image: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blf32MK7o72w",
        "outputId": "0f0251b3-ca22-4502-9d22-4490988ab5a2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-47.980366, -46.823513, -32.056293, -51.02874 , -44.4783  ,\n",
              "        -17.258514, -45.40839 , -12.528049, -41.006466,   8.104328]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_test[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBQVDMbZjl6I",
        "outputId": "3fbc39db-c0f5-430a-deed-f449b1e9f1f0"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_predictions = model.predict(img_test[:10])  # Input shape (10, 28, 28)\n",
        "predicted_classes = np.argmax(batch_predictions, axis=1)  # Class for each image\n",
        "print(predicted_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25Z12dzvjmb7",
        "outputId": "ecf817f8-982c-4cfc-bfb0-e12ec16a6d34"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "[9 2 1 1 0 1 4 6 5 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import keras_tuner as kt\n",
        "\n",
        "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Normalize pixel values between 0 and 1\n",
        "img_train = img_train.astype('float32') / 255.0\n",
        "img_test = img_test.astype('float32') / 255.0\n",
        "\n",
        "\n",
        "def model_builder(hp):\n",
        "  model = keras.Sequential()\n",
        "  #input layers\n",
        "  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
        "\n",
        "  # Tune the number of units in the first Dense layer\n",
        "  # Choose an optimal value between 32-512\n",
        "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "  model.add(keras.layers.Dense(10))#output layer\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "  tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='intro_to_kt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  tuner.search(img_train, label_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "\"\"\" output = Trial 30 Complete [00h 02m 04s]\n",
        "val_accuracy: 0.890250027179718\n",
        "\n",
        "Best val_accuracy So Far: 0.8924166560173035\n",
        "Total elapsed time: 00h 18m 52s\n",
        "\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is 512 and the optimal learning rate for the optimizer\n",
        "is 0.001. \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(img_train, label_train, epochs=50, validation_split=0.2)\n",
        "\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))\n",
        "\n",
        "\n",
        "\n",
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Retrain the model\n",
        "hypermodel.fit(img_train, label_train, epochs=best_epoch, validation_split=0.2)\n",
        "\n",
        "\n",
        "\n",
        "eval_result = model.evaluate(img_test, label_test)\n",
        "print(\"[test loss, test accuracy]:\", eval_result)"
      ],
      "metadata": {
        "id": "D90C_3u7dSLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8z8wupNejipy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}